============
Installation
============

# Installation instructions for building Chronicling America on Ubuntu. 
# First you'll need to get the latest Chronicling America code. Which
# will include this file.

System Dependencies
===================

sudo apt-get install python-dev python-virtualenv mysql-server libmysqlclient-dev apache2 libapache2-mod-wsgi solr-jetty openjdk-6-jdk libxml2-dev libxslt-dev libjpeg-dev git-core graphicsmagick

# Note: When you install mysql-server, you will be prompted for a root 
# password. If you choose one, make a note of what it is. Later you will be 
# asked to enter the password when you create the database for the project.

Get Chronam
===========

mkdir /opt/chronam
git clone https://rdc.lctl.gov/git/chronam.git /opt/chronam
export CHRONAM_HOME=/opt/chronam
cd ${CHRONAM_HOME}


Create Database
===============

echo "DROP DATABASE IF EXISTS chronam; CREATE DATABASE chronam CHARACTER SET utf8; GRANT ALL ON chronam.* to 'chronam'@'localhost' identified by 'pick_one'; GRANT ALL ON test_chronam.* TO 'chronam'@'localhost' identified by 'pick_one';" | mysql -u root -p


Configure Solr
==============

sudo cp ${CHRONAM_HOME}/conf/schema.xml /etc/solr/conf/
sudo cp ${CHRONAM_HOME}/conf/solrconfig-ubuntu.xml /etc/solr/conf/solrconfig.xml
sudo ln -s /usr/share/java/lucene-memory.jar /usr/share/solr/WEB-INF/lib/lucene-memory.jar 
# edit /etc/default/jetty and change NO_START to 0 
sudo service jetty start


Configure Apache
================

sudo a2enmod cache expires rewrite disk_cache
sudo cp conf/chronam.conf /etc/apache2/sites-available/chronam
sudo a2ensite chronam

sudo install -o `whoami` -g users -d ${CHRONAM_HOME}/static
sudo install -o `whoami` -g users -d ${CHRONAM_HOME}/.python-eggs


Configure Chronam
=================

virtualenv --no-site-packages ${CHRONAM_HOME}/ENV
source ${CHRONAM_HOME}/ENV/bin/activate
add2virtualenv /opt
export DJANGO_SETTINGS_MODULE=chronam.settings

# install python dependencies
pip install -r requirements.pip

# create directories for data
mkdir ${CHRONAM_HOME}/data/batches
mkdir ${CHRONAM_HOME}/data/cache
mkdir ${CHRONAM_HOME}/data/bib

# create settings file, and set the mysql username/password
cp ${CHRONAM_HOME}/settings_template.py settings.py

# set up the database and load initial data
django-admin.py syncdb --noinput --migrate
django-admin.py chronam_sync


Start Up Chronicling America
============================

sudo service apache2 restart


Get Sample Data / Run Unit Tests
================================

# Sample data set 1: batch_vi_affirmed_ver01
# The first dataset is provided to be a quick and easy download in order for you to understand how the project works.
cd ${CHRONAM_HOME}/data/batches
wget --recursive --no-host-directories --cut-dirs 1 --include-directories /data/vi/batch_vi_affirmed_ver01 http://chroniclingamerica.loc.gov/data/vi/batch_vi_affirmed_ver01

# Sample data set 2: batch_dlc_jamaica_ver01
# The second dataset is provided to run tests against.
# Beware this will pull down 60G of data, which could take hours to days to download depending on your connection speed.
# Make sure you have a stable connection and enough room on your harddrive.
cd ${CHRONAM_HOME}/data/batches
wget --recursive --no-host-directories --cut-dirs 1 --include-directories /data/dlc/batch_dlc_jamaica_ver01 http://chroniclingamerica.loc.gov/data/dlc/batch_dlc_jamaica_ver01


Run Unit Tests
==============

# Currently chronam.core.tests.batch_loader_tests.BatchLoaderTest checks batch_dlc_jamaica_ver01
# If you grabbed the smaller sample set, the tests will fail.
django-admin.py test core


Loading the Data
================

sudo install -o `whoami` -g users -d ${CHRONAM_HOME}/logs/
cd ${CHRONAM_HOME}/logs/

# or load a specific batch, replace $BATCH_NAME with a specific batch.
django-admin.py load_batch $BATCH_NAME
# Example: django-admin.py load_batch batch_vi_affirmed_ver01

# load up links to flickr using your flickr key
django-admin.py flickr $YOUR_FLICKR_KEY
